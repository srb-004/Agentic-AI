{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6805d0",
   "metadata": {},
   "source": [
    "### Pine cone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b69c241c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d26347d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGING_FACE_TOKEN\"] = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ea5093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bharath\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding1= HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7c8967",
   "metadata": {},
   "source": [
    "#### Using Google embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e405b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "len(google_embeddings.embed_query(\"Hello World\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c74df7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding1.embed_query(\"Hi Bharath\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ca2414",
   "metadata": {},
   "source": [
    "### Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2974669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93654ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"index-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "475943e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not pc.has_index(index):\n",
    "    pc.create_index(\n",
    "        name=index,\n",
    "        dimension=768,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31ae4ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.data.index.Index at 0x179e334f810>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name = pc.Index(index)\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1ac5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48bed7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PineconeVectorStore(index=index_name, embedding=google_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4f803c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "document_1 = Document(\n",
    "    page_content=\"I had chocolate chip pancakes and scrambled eggs for breakfast this morning.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "document_2 = Document(\n",
    "    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_3 = Document(\n",
    "    page_content=\"Building an exciting new project with LangChain - come check it out!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_4 = Document(\n",
    "    page_content=\"Robbers broke into the city bank and stole $1 million in cash.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_5 = Document(\n",
    "    page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_6 = Document(\n",
    "    page_content=\"Is the new iPhone worth the price? Read this review to find out.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_7 = Document(\n",
    "    page_content=\"The top 10 soccer players in the world right now.\",\n",
    "    metadata={\"source\": \"website\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_8 = Document(\n",
    "    page_content=\"LangGraph is the best framework for building stateful, agentic applications!\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_9 = Document(\n",
    "    page_content=\"The stock market is down 500 points today due to fears of a recession.\",\n",
    "    metadata={\"source\": \"news\"},\n",
    ")\n",
    "\n",
    "\n",
    "document_10 = Document(\n",
    "    page_content=\"I have a bad feeling I am going to get deleted :(\",\n",
    "    metadata={\"source\": \"tweet\"},\n",
    ")\n",
    "\n",
    "\n",
    "documents = [\n",
    "    document_1,\n",
    "    document_2,\n",
    "    document_3,\n",
    "    document_4,\n",
    "    document_5,\n",
    "    document_6,\n",
    "    document_7,\n",
    "    document_8,\n",
    "    document_9,\n",
    "    document_10,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83467b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'tweet'}, page_content='I had chocolate chip pancakes and scrambled eggs for breakfast this morning.'),\n",
       " Document(metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!'),\n",
       " Document(metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content=\"Wow! That was an amazing movie. I can't wait to see it again.\"),\n",
       " Document(metadata={'source': 'website'}, page_content='Is the new iPhone worth the price? Read this review to find out.'),\n",
       " Document(metadata={'source': 'website'}, page_content='The top 10 soccer players in the world right now.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='LangGraph is the best framework for building stateful, agentic applications!'),\n",
       " Document(metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'),\n",
       " Document(metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :(')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0e28833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "uuids = [str(uuid4()) for i in range(len(documents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab9d25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['79c788a4-448c-4f64-83bc-eb075eda76f2', '42bdec08-012d-4815-9ba3-68f04296d814', '0e265470-97ea-404c-a2e9-ab89bac07ec0', '190b3d50-91ec-48b7-bd0b-492d58077aef', '5e9eff41-0fc1-4efb-bf7c-65c221630fa0', '02191078-caa0-4b71-8a0c-c254b65cc2c8', '45b61a53-1314-439d-ab2d-c074ab690914', 'f15fb443-bd71-48b2-bf99-71cc29cb2552', '344bf992-76e3-4447-98c2-b8bc715a21c1', 'dd18b3b9-b303-49ae-bafb-38637266732d']\n"
     ]
    }
   ],
   "source": [
    "print(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df38bfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['79c788a4-448c-4f64-83bc-eb075eda76f2',\n",
       " '42bdec08-012d-4815-9ba3-68f04296d814',\n",
       " '0e265470-97ea-404c-a2e9-ab89bac07ec0',\n",
       " '190b3d50-91ec-48b7-bd0b-492d58077aef',\n",
       " '5e9eff41-0fc1-4efb-bf7c-65c221630fa0',\n",
       " '02191078-caa0-4b71-8a0c-c254b65cc2c8',\n",
       " '45b61a53-1314-439d-ab2d-c074ab690914',\n",
       " 'f15fb443-bd71-48b2-bf99-71cc29cb2552',\n",
       " '344bf992-76e3-4447-98c2-b8bc715a21c1',\n",
       " 'dd18b3b9-b303-49ae-bafb-38637266732d']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add_documents(documents=documents,ids = uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cabdf6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='344bf992-76e3-4447-98c2-b8bc715a21c1', metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'), Document(id='42bdec08-012d-4815-9ba3-68f04296d814', metadata={'source': 'news'}, page_content='The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.'), Document(id='190b3d50-91ec-48b7-bd0b-492d58077aef', metadata={'source': 'news'}, page_content='Robbers broke into the city bank and stole $1 million in cash.')]\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\"What is langchain?\", k= 3, filter={\"source\" : \"news\"})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "168ed98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type = \"similarity_score_threshold\", \n",
    "                                      search_kwargs={\"k\":3, \"score_threshold\": 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a61e612f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dd18b3b9-b303-49ae-bafb-38637266732d', metadata={'source': 'tweet'}, page_content='I have a bad feeling I am going to get deleted :('),\n",
       " Document(id='344bf992-76e3-4447-98c2-b8bc715a21c1', metadata={'source': 'news'}, page_content='The stock market is down 500 points today due to fears of a recession.'),\n",
       " Document(id='0e265470-97ea-404c-a2e9-ab89bac07ec0', metadata={'source': 'tweet'}, page_content='Building an exciting new project with LangChain - come check it out!')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Delete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c619f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"models/gemini-1.5-pro-latest\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5e196e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "db4e2ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8de8bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Context: {context} Question: {question} Answer:\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c5d76b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97275f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7de68fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:264\u001b[39m, in \u001b[36membed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1305\u001b[39m, in \u001b[36membed_content\u001b[39m\u001b[34m(self, request, model, content, retry, timeout, metadata)\u001b[39m\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbatch_embed_contents\u001b[39m(\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1275\u001b[39m     request: Optional[\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     metadata: Sequence[Tuple[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]]] = (),\n\u001b[32m   1286\u001b[39m ) -> generative_service.BatchEmbedContentsResponse:\n\u001b[32m   1287\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Generates multiple embedding vectors from the input ``Content``\u001b[39;00m\n\u001b[32m   1288\u001b[39m \u001b[33;03m    which consists of a batch of strings represented as\u001b[39;00m\n\u001b[32m   1289\u001b[39m \u001b[33;03m    ``EmbedContentRequest`` objects.\u001b[39;00m\n\u001b[32m   1290\u001b[39m \n\u001b[32m   1291\u001b[39m \u001b[33;03m    .. code-block:: python\u001b[39;00m\n\u001b[32m   1292\u001b[39m \n\u001b[32m   1293\u001b[39m \u001b[33;03m        # This snippet has been automatically generated and should be regarded as a\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[33;03m        # code template only.\u001b[39;00m\n\u001b[32m   1295\u001b[39m \u001b[33;03m        # It will require modifications to work:\u001b[39;00m\n\u001b[32m   1296\u001b[39m \u001b[33;03m        # - It may require correct/in-range values for request initialization.\u001b[39;00m\n\u001b[32m   1297\u001b[39m \u001b[33;03m        # - It may require specifying regional endpoints when creating the service\u001b[39;00m\n\u001b[32m   1298\u001b[39m \u001b[33;03m        #   client as shown in:\u001b[39;00m\n\u001b[32m   1299\u001b[39m \u001b[33;03m        #   https://googleapis.dev/python/google-api-core/latest/client_options.html\u001b[39;00m\n\u001b[32m   1300\u001b[39m \u001b[33;03m        from google.ai import generativelanguage_v1beta\u001b[39;00m\n\u001b[32m   1301\u001b[39m \n\u001b[32m   1302\u001b[39m \u001b[33;03m        def sample_batch_embed_contents():\u001b[39;00m\n\u001b[32m   1303\u001b[39m \u001b[33;03m            # Create a client\u001b[39;00m\n\u001b[32m   1304\u001b[39m \u001b[33;03m            client = generativelanguage_v1beta.GenerativeServiceClient()\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1305\u001b[39m \n\u001b[32m   1306\u001b[39m \u001b[33;03m            # Initialize request argument(s)\u001b[39;00m\n\u001b[32m   1307\u001b[39m \u001b[33;03m            requests = generativelanguage_v1beta.EmbedContentRequest()\u001b[39;00m\n\u001b[32m   1308\u001b[39m \u001b[33;03m            requests.model = \"model_value\"\u001b[39;00m\n\u001b[32m   1309\u001b[39m \n\u001b[32m   1310\u001b[39m \u001b[33;03m            request = generativelanguage_v1beta.BatchEmbedContentsRequest(\u001b[39;00m\n\u001b[32m   1311\u001b[39m \u001b[33;03m                model=\"model_value\",\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[33;03m                requests=requests,\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[33;03m            )\u001b[39;00m\n\u001b[32m   1314\u001b[39m \n\u001b[32m   1315\u001b[39m \u001b[33;03m            # Make the request\u001b[39;00m\n\u001b[32m   1316\u001b[39m \u001b[33;03m            response = client.batch_embed_contents(request=request)\u001b[39;00m\n\u001b[32m   1317\u001b[39m \n\u001b[32m   1318\u001b[39m \u001b[33;03m            # Handle the response\u001b[39;00m\n\u001b[32m   1319\u001b[39m \u001b[33;03m            print(response)\u001b[39;00m\n\u001b[32m   1320\u001b[39m \n\u001b[32m   1321\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   1322\u001b[39m \u001b[33;03m        request (Union[google.ai.generativelanguage_v1beta.types.BatchEmbedContentsRequest, dict]):\u001b[39;00m\n\u001b[32m   1323\u001b[39m \u001b[33;03m            The request object. Batch request to get embeddings from\u001b[39;00m\n\u001b[32m   1324\u001b[39m \u001b[33;03m            the model for a list of prompts.\u001b[39;00m\n\u001b[32m   1325\u001b[39m \u001b[33;03m        model (str):\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[33;03m            Required. The model's resource name. This serves as an\u001b[39;00m\n\u001b[32m   1327\u001b[39m \u001b[33;03m            ID for the Model to use.\u001b[39;00m\n\u001b[32m   1328\u001b[39m \n\u001b[32m   1329\u001b[39m \u001b[33;03m            This name should match a model name returned by the\u001b[39;00m\n\u001b[32m   1330\u001b[39m \u001b[33;03m            ``ListModels`` method.\u001b[39;00m\n\u001b[32m   1331\u001b[39m \n\u001b[32m   1332\u001b[39m \u001b[33;03m            Format: ``models/{model}``\u001b[39;00m\n\u001b[32m   1333\u001b[39m \n\u001b[32m   1334\u001b[39m \u001b[33;03m            This corresponds to the ``model`` field\u001b[39;00m\n\u001b[32m   1335\u001b[39m \u001b[33;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[32m   1336\u001b[39m \u001b[33;03m            should not be set.\u001b[39;00m\n\u001b[32m   1337\u001b[39m \u001b[33;03m        requests (MutableSequence[google.ai.generativelanguage_v1beta.types.EmbedContentRequest]):\u001b[39;00m\n\u001b[32m   1338\u001b[39m \u001b[33;03m            Required. Embed requests for the batch. The model in\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[33;03m            each of these requests must match the model specified\u001b[39;00m\n\u001b[32m   1340\u001b[39m \u001b[33;03m            ``BatchEmbedContentsRequest.model``.\u001b[39;00m\n\u001b[32m   1341\u001b[39m \n\u001b[32m   1342\u001b[39m \u001b[33;03m            This corresponds to the ``requests`` field\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m            on the ``request`` instance; if ``request`` is provided, this\u001b[39;00m\n\u001b[32m   1344\u001b[39m \u001b[33;03m            should not be set.\u001b[39;00m\n\u001b[32m   1345\u001b[39m \u001b[33;03m        retry (google.api_core.retry.Retry): Designation of what errors, if any,\u001b[39;00m\n\u001b[32m   1346\u001b[39m \u001b[33;03m            should be retried.\u001b[39;00m\n\u001b[32m   1347\u001b[39m \u001b[33;03m        timeout (float): The timeout for this request.\u001b[39;00m\n\u001b[32m   1348\u001b[39m \u001b[33;03m        metadata (Sequence[Tuple[str, Union[str, bytes]]]): Key/value pairs which should be\u001b[39;00m\n\u001b[32m   1349\u001b[39m \u001b[33;03m            sent along with the request as metadata. Normally, each value must be of type `str`,\u001b[39;00m\n\u001b[32m   1350\u001b[39m \u001b[33;03m            but for metadata keys ending with the suffix `-bin`, the corresponding values must\u001b[39;00m\n\u001b[32m   1351\u001b[39m \u001b[33;03m            be of type `bytes`.\u001b[39;00m\n\u001b[32m   1352\u001b[39m \n\u001b[32m   1353\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m   1354\u001b[39m \u001b[33;03m        google.ai.generativelanguage_v1beta.types.BatchEmbedContentsResponse:\u001b[39;00m\n\u001b[32m   1355\u001b[39m \u001b[33;03m            The response to a BatchEmbedContentsRequest.\u001b[39;00m\n\u001b[32m   1356\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1357\u001b[39m     \u001b[38;5;66;03m# Create or coerce a protobuf request object.\u001b[39;00m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;66;03m# - Quick check: If we got a request object, we should *not* have\u001b[39;00m\n\u001b[32m   1359\u001b[39m     \u001b[38;5;66;03m#   gotten any keyword arguments that map to the request.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    207\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    208\u001b[39m         error_list,\n\u001b[32m    209\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    210\u001b[39m         original_timeout,\n\u001b[32m    211\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is langchain?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = \u001b[43m{\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3774\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3769\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3770\u001b[39m         futures = [\n\u001b[32m   3771\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3772\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3773\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3775\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3776\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3758\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3752\u001b[39m child_config = patch_config(\n\u001b[32m   3753\u001b[39m     config,\n\u001b[32m   3754\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3755\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3756\u001b[39m )\n\u001b[32m   3757\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context.run(\n\u001b[32m   3759\u001b[39m         step.invoke,\n\u001b[32m   3760\u001b[39m         input_,\n\u001b[32m   3761\u001b[39m         child_config,\n\u001b[32m   3762\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\retrievers.py:259\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    257\u001b[39m _kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1082\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m     docs = \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search(query, **_kwargs)\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1081\u001b[39m     docs_and_similarities = (\n\u001b[32m-> \u001b[39m\u001b[32m1082\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_kwargs\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1085\u001b[39m     )\n\u001b[32m   1086\u001b[39m     docs = [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities]\n\u001b[32m   1087\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:556\u001b[39m, in \u001b[36mVectorStore.similarity_search_with_relevance_scores\u001b[39m\u001b[34m(self, query, k, **kwargs)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return docs and relevance scores in the range [0, 1].\u001b[39;00m\n\u001b[32m    541\u001b[39m \n\u001b[32m    542\u001b[39m \u001b[33;03m0 is dissimilar, 1 is most similar.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m \u001b[33;03m    List of Tuples of (doc, similarity_score).\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    554\u001b[39m score_threshold = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mscore_threshold\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m docs_and_similarities = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_similarity_search_with_relevance_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    560\u001b[39m     similarity < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m similarity > \u001b[32m1.0\u001b[39m\n\u001b[32m    561\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _, similarity \u001b[38;5;129;01min\u001b[39;00m docs_and_similarities\n\u001b[32m    562\u001b[39m ):\n\u001b[32m    563\u001b[39m     warnings.warn(\n\u001b[32m    564\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRelevance scores must be between\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m 0 and 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdocs_and_similarities\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    566\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    567\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:504\u001b[39m, in \u001b[36mVectorStore._similarity_search_with_relevance_scores\u001b[39m\u001b[34m(self, query, k, **kwargs)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Default similarity search with relevance scores.\u001b[39;00m\n\u001b[32m    487\u001b[39m \n\u001b[32m    488\u001b[39m \u001b[33;03mModify if necessary in subclass.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    501\u001b[39m \u001b[33;03m    List of Tuples of (doc, similarity_score)\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    503\u001b[39m relevance_score_fn = \u001b[38;5;28mself\u001b[39m._select_relevance_score_fn()\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [(doc, relevance_score_fn(score)) \u001b[38;5;28;01mfor\u001b[39;00m doc, score \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:409\u001b[39m, in \u001b[36mPineconeVectorStore.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, namespace)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search_with_score\u001b[39m(\n\u001b[32m    391\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    392\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     namespace: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    396\u001b[39m ) -> List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    397\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return pinecone documents most similar to query, along with scores.\u001b[39;00m\n\u001b[32m    398\u001b[39m \n\u001b[32m    399\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    406\u001b[39m \u001b[33;03m        List of Documents most similar to the query and score for each\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity_search_by_vector_with_score(\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m, k=k, \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m, namespace=namespace\n\u001b[32m    410\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\miniconda\\envs\\agenticvenv\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:266\u001b[39m, in \u001b[36membed_query\u001b[39m\u001b[34m(self, text, task_type, title, output_dimensionality)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[31mGoogleGenerativeAIError\u001b[39m: Error embedding content: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]"
     ]
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c9b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
